{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 细粒度用户情感分析_个人baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先是数据预处理的工作，简单的清洗，然后分词，训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "数据预处理，清洗，分词，训练词向量\n",
    "\n",
    "@author: yuhaitao\n",
    "\"\"\"\n",
    "# -*- coding:utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import os\n",
    "import word2vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data_from_csv(file_name, header=0, encoding=\"utf-8\"):\n",
    "    \"\"\"\n",
    "    加载数据成dataframe\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name, header=header, encoding=encoding)\n",
    "    return df\n",
    "\n",
    "def data_clean(dataframe, out_path, isTest=False):\n",
    "    \"\"\"\n",
    "    数据清洗，并保存清洗后的数据\n",
    "    \"\"\"\n",
    "    stop_words = ['.', '，', '…', '、', ' ', '\\n', '\\t', '~'] # 停用词表还需要扩充\n",
    "    new_dataframe = pd.DataFrame(columns=dataframe.columns)\n",
    "    for index, rows in tqdm(dataframe.iterrows()):\n",
    "        sentence = rows[\"content\"]\n",
    "        new_sentence = \"\"\n",
    "        # 清洗评论\n",
    "        for word in sentence:\n",
    "            if word not in stop_words:\n",
    "                new_sentence += word\n",
    "        rows[\"content\"] = new_sentence\n",
    "        # 清洗标签\n",
    "        if not isTest:\n",
    "            for column, value in rows.iteritems():\n",
    "                if column != \"content\":\n",
    "                    if np.isnan(float(rows[column])) == True:\n",
    "                        rows[column] = -2\n",
    "                        print(\"遇到空值:{}\".format(index))\n",
    "        new_dataframe.loc[index] = rows\n",
    "    print(\"清洗完成！\")\n",
    "    # 存储新的dataframe\n",
    "    new_dataframe.to_csv(out_path, index=None)\n",
    "    return new_dataframe\n",
    "    \n",
    "\n",
    "def seg_words(contents):\n",
    "    \"\"\"\n",
    "    分词\n",
    "    \"\"\"\n",
    "    contents_segs = []\n",
    "    for content in contents:\n",
    "        segs = jieba.cut(content)\n",
    "        contents_segs.append(\" \".join(segs))\n",
    "    return contents_segs\n",
    "\n",
    "train_file = \"./sentiment_analysis_trainingset.csv\"\n",
    "validation_file = \"./sentiment_analysis_validationset.csv\"\n",
    "test_file = \"./sentiment_analysis_testa.csv\"\n",
    "train_after_clean = \"./train_after_clean.csv\"\n",
    "val_after_clean = \"./val_after_clean.csv\"\n",
    "test_after_clean = \"./test_after_clean.csv\"\n",
    "seg_txt = \"./seg_list.txt\"\n",
    "embedding_bin = \"./embedding.bin\"\n",
    "content_limit = 500\n",
    "\n",
    "def preprocess():\n",
    "    \"\"\"\n",
    "    数据预处理函数\n",
    "    \"\"\"\n",
    "    train_df = load_data_from_csv(train_file)\n",
    "    val_df = load_data_from_csv(validation_file)\n",
    "    test_df = load_data_from_csv(test_file)\n",
    "    \n",
    "    # 数据清洗\n",
    "    train_df = data_clean(train_df, train_after_clean)\n",
    "    val_df = data_clean(val_df, val_after_clean)\n",
    "    test_df = data_clean(test_df, test_after_clean, isTest=True)\n",
    "    \n",
    "    train_content = train_df.iloc[:,1]\n",
    "    val_content = val_df.iloc[:,1]\n",
    "    test_content = test_df.iloc[:,1]\n",
    "    \n",
    "    # 分词，构造语料库\n",
    "    all_content = []\n",
    "    all_content.extend(train_content)\n",
    "    all_content.extend(val_content)\n",
    "    all_content.extend(test_content)\n",
    "    print(len(all_content))\n",
    "    all_seg_words = seg_words(all_content)\n",
    "    with open(seg_txt, \"w+\") as txt_write:\n",
    "        for sentence in tqdm(all_seg_words):\n",
    "            sentence = sentence.replace(\"\\n\",\"\") + \"\\n\"\n",
    "            txt_write.write(sentence)\n",
    "    txt_write.close()\n",
    "    \n",
    "    # 调用word2vec\n",
    "    word2vec.word2vec(seg_txt, embedding_bin, min_count=5, size=100, verbose=True)\n",
    "    \n",
    "\n",
    "# 数据预处理\n",
    "# preprocess()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过数据预处理后，接下来需要生成dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nge = batch_generator(get_dataset(val_after_clean)[:5],2,embedding_bin)\\nfor i in range(6):\\n    aa, bb, cc = next(ge)\\n    print(aa)\\n    print(bb)\\n    print(cc)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "生成dataset以备训练与测试使用\n",
    "\n",
    "@author: yuhaitao\n",
    "\"\"\"\n",
    "# -*- coding:utf-8 -*-\n",
    "import random\n",
    "\n",
    "def get_passage_limit(): # 最长3000多，还需清洗\n",
    "    train_df = load_data_from_csv(train_after_clean)\n",
    "    val_df = load_data_from_csv(val_after_clean)\n",
    "    test_df = load_data_from_csv(test_after_clean)\n",
    "    train_content = train_df.iloc[:,1]\n",
    "    val_content = val_df.iloc[:,1]\n",
    "    test_content = test_df.iloc[:,1]\n",
    "    all_content = []\n",
    "    all_content.extend(train_content)\n",
    "    all_content.extend(val_content)\n",
    "    all_content.extend(test_content)\n",
    "    print(len(all_content))\n",
    "    all_seg_words = seg_words(all_content)\n",
    "    max = 0\n",
    "    max_sen = []\n",
    "    for sentence in all_seg_words:\n",
    "        sentence = sentence.replace(\"\\n\",\"\") + \"\\n\"\n",
    "        if len(sentence) > max:\n",
    "            max = len(sentence)\n",
    "            max_sen = sentence\n",
    "    return max, max_sen\n",
    "    \n",
    "\n",
    "def get_dataset(file): # 最长的一个1302,我们设置长度限制为500\n",
    "    \"\"\"\n",
    "    生成train和validation的dataset，ids：数据的id号，x：content分词列表，y:标签列表\n",
    "    \"\"\"\n",
    "    dataframe = load_data_from_csv(file)\n",
    "    dataset = []\n",
    "    for index, rows in dataframe.iterrows():\n",
    "        ids = rows[\"id\"]\n",
    "        x = list(jieba.cut(rows[\"content\"]))\n",
    "        if len(x) > content_limit:\n",
    "            x = x[:content_limit]\n",
    "        y = list(rows[2:])\n",
    "        dataset.append({\"id\":ids,\"content\":x,\"labels\":y})\n",
    "    return dataset\n",
    "        \n",
    "    \n",
    "def batch_generator(dataset, batch_size, word2vec_bin, shuffle=True):\n",
    "    \"\"\"\n",
    "    batch生成器\n",
    "    \"\"\"\n",
    "    word2vec_model = word2vec.load(word2vec_bin)\n",
    "    if shuffle:\n",
    "        random.shuffle(dataset)\n",
    "    data_num = len(dataset)\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        if batch_count * batch_size + batch_size > data_num:\n",
    "            # 最后一个batch的操作\n",
    "            one_batch = dataset[batch_count * batch_size:data_num]\n",
    "            for i in range(batch_size + batch_count * batch_size - data_num):\n",
    "                one_batch.append(dataset[i])\n",
    "            batch_count = 0\n",
    "            if shuffle:\n",
    "                random.shuffle(dataset)\n",
    "        else:\n",
    "            one_batch = dataset[batch_count * batch_size:batch_count * batch_size + batch_size]\n",
    "            batch_count += 1\n",
    "        # 提取数据\n",
    "        index = 0\n",
    "        one_batch_ids = []\n",
    "        one_batch_inputs = np.zeros(shape=[batch_size, content_limit, 100], dtype=np.float32)\n",
    "        one_batch_labels = []\n",
    "        for one in one_batch:\n",
    "            one_batch_ids.append(one[\"id\"])\n",
    "            one_batch_labels.append(one[\"labels\"])\n",
    "            for i in range(len(one[\"content\"])):\n",
    "                if one[\"content\"][i] in word2vec_model:\n",
    "                    one_batch_inputs[index,i,:] = word2vec_model[one[\"content\"][i]] # 这里会出现keyerror，以后要注意一下\n",
    "            index += 1\n",
    "        one_batch_ids = np.array(one_batch_ids)\n",
    "        one_batch_labels = np.array(one_batch_labels)\n",
    "        yield one_batch_ids, one_batch_inputs, one_batch_labels \n",
    "        \n",
    "            \n",
    "                \n",
    "# get_passage_limit()\n",
    "\n",
    "\"\"\"\n",
    "ge = batch_generator(get_dataset(val_after_clean)[:5],2,embedding_bin)\n",
    "for i in range(6):\n",
    "    aa, bb, cc = next(ge)\n",
    "    print(aa)\n",
    "    print(bb)\n",
    "    print(cc)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "训练模块\n",
    "\n",
    "@author: yuhaitao\n",
    "\"\"\"\n",
    "# -*- coding:utf-8 -*-\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "save_dir = \"./model/\"\n",
    "\n",
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    标签数据规范化成（0，1）\n",
    "    \"\"\"\n",
    "    data = data.astype(np.float32)\n",
    "    max = data.max()\n",
    "    min = data.min()\n",
    "    data = (data - min) / (max - min)\n",
    "    return data\n",
    "\n",
    "def predict(logits):\n",
    "    \"\"\"\n",
    "    根据神经网络输出预处真实标签\n",
    "    \"\"\"\n",
    "    for i in range(logits.shape[0]):\n",
    "        for j in range(logits.shape[1]):\n",
    "            if logits[i][j] < 0.5:\n",
    "                logits[i][j] = -2\n",
    "            elif logits[i][j] < 0.65:\n",
    "                logits[i][j] = -1\n",
    "            elif logits[i][j] < 0.8:\n",
    "                logits[i][j] = 0\n",
    "            else:\n",
    "                logits[i][j] = 1\n",
    "    logits = logits.astype(np.int32)\n",
    "    return logits\n",
    "        \n",
    "\n",
    "def f1_score_from_sklearn(predictions_dict, targets_dict):\n",
    "    \"\"\"\n",
    "    计算F1得分\n",
    "    \"\"\"\n",
    "    f1_list = []\n",
    "    for keys, value in predictions_dict.items():\n",
    "        tar = targets_dict[keys]\n",
    "        f1_list.append(f1_score(tar, value, average=\"macro\"))\n",
    "    f1_list = np.array(f1_list)\n",
    "    return np.mean(f1_list)\n",
    "    \n",
    "\n",
    "def f1_score_bymyself(predictions_dict, targets_dict):\n",
    "    \"\"\"\n",
    "    自己计算F1得分\n",
    "    \"\"\"\n",
    "\n",
    "def dense(inputs, hidden, use_bias=True, scope=\"dense\"):\n",
    "    \"\"\"\n",
    "    全连接层\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        shape = tf.shape(inputs)\n",
    "        dim = inputs.get_shape().as_list()[-1]\n",
    "        out_shape = [shape[idx] for idx in range(\n",
    "            len(inputs.get_shape().as_list()) - 1)] + [hidden]\n",
    "        flat_inputs = tf.reshape(inputs, [-1, dim])\n",
    "        W = tf.get_variable(\"W\", [dim, hidden])\n",
    "        res = tf.matmul(flat_inputs, W)\n",
    "        if use_bias:\n",
    "            b = tf.get_variable(\n",
    "                \"b\", [hidden], initializer=tf.constant_initializer(0.))\n",
    "            res = tf.nn.bias_add(res, b)\n",
    "        res = tf.reshape(res, out_shape)\n",
    "        return res\n",
    "\n",
    "\n",
    "def train(batch_size=64, hidden=60, grad_clip=5.0, init_lr=1.0):\n",
    "    \"\"\"\n",
    "    训练与验证的函数\n",
    "    \"\"\"\n",
    "    # 搭建模型\n",
    "    print(\"Building model...\")\n",
    "    inputs = tf.placeholder(dtype=tf.float32, shape=[batch_size, content_limit, 100], name=\"inputs\")\n",
    "    labels = tf.placeholder(dtype=tf.float32, shape=[batch_size, 20], name=\"labels\")\n",
    "    keep_prob = tf.placeholder(dtype=tf.float32, name=\"keep_prob\")\n",
    "    global_step = tf.get_variable(name='global_step', shape=[], dtype=tf.int32, initializer=tf.constant_initializer(0), trainable=False)\n",
    "    # 做mask处理\n",
    "    mask = tf.cast(inputs[:,:,0], tf.bool)\n",
    "    seq_len = tf.reduce_sum(tf.cast(mask, tf.int32), axis=1)\n",
    "    max_len = tf.reduce_max(seq_len)\n",
    "    mask = tf.slice(mask, [0,0], [batch_size, max_len])\n",
    "    seq_f = tf.slice(inputs, [0,0,0], [batch_size, max_len, 100])\n",
    "    seq_b = tf.reverse_sequence(seq_f, seq_lengths=seq_len, seq_dim=1, batch_dim=0)\n",
    "    # 双向 GRU layer, f:forward, b:backward\n",
    "    gru_f = tf.contrib.rnn.GRUCell(hidden,name=\"gru_f\")\n",
    "    gru_b = tf.contrib.rnn.GRUCell(hidden,name=\"gru_b\")\n",
    "    init_f = tf.tile(tf.Variable(tf.zeros([1, hidden]), name=\"init_f\"), [batch_size, 1])\n",
    "    init_b = tf.tile(tf.Variable(tf.zeros([1, hidden]), name=\"init_b\"), [batch_size, 1])\n",
    "    with tf.variable_scope(\"BiGRU\"):\n",
    "        seq_f = tf.nn.dropout(seq_f, keep_prob)\n",
    "        seq_b = tf.nn.dropout(seq_b, keep_prob)\n",
    "        f, f_state = tf.nn.dynamic_rnn(gru_f, seq_f, seq_len, initial_state=init_f, dtype=tf.float32)\n",
    "        b_, b_state = tf.nn.dynamic_rnn(gru_b, seq_b, seq_len, initial_state=init_b, dtype=tf.float32)\n",
    "        b = tf.reverse_sequence(b_, seq_lengths=seq_len, seq_dim=1, batch_dim=0)\n",
    "        gru_out = tf.nn.dropout(tf.concat([f, b], axis=2), keep_prob=keep_prob)\n",
    "    with tf.variable_scope(\"attention\"):\n",
    "        att_inputs = tf.nn.relu(dense(gru_out, hidden, use_bias=False, scope=\"att_inputs\"))\n",
    "        att_memory = tf.nn.relu(dense(gru_out, hidden, use_bias=False, scope=\"att_memory\"))\n",
    "        att_1 = tf.matmul(att_inputs, tf.transpose(att_memory, [0,2,1])) / (hidden ** 0.5)\n",
    "        att_mask = tf.tile(tf.expand_dims(mask, axis=1), [1,tf.shape(att_inputs)[1],1])\n",
    "        att_softmax_mask = att_1 - (1e30 * (1 - tf.cast(att_mask, tf.float32)))\n",
    "        att_logits = tf.nn.softmax(att_softmax_mask)\n",
    "        att_out = tf.nn.dropout(tf.matmul(att_logits, gru_out), keep_prob=keep_prob)\n",
    "    with tf.variable_scope(\"pooling\"):\n",
    "        po_1 = tf.nn.tanh(dense(att_out, hidden, scope=\"po_1\"))\n",
    "        po_2 = dense(po_1, 1, use_bias=False, scope=\"po_2\")\n",
    "        po_softmax_mask = tf.squeeze(po_2,[2]) - (1e30 * (1 - tf.cast(mask, tf.float32)))\n",
    "        po_weight = tf.expand_dims(tf.nn.softmax(po_softmax_mask),axis=2)\n",
    "        po_out = tf.reduce_sum(po_weight * att_out, axis=1)\n",
    "    with tf.variable_scope(\"fully_connect\"):\n",
    "        w = tf.get_variable(name=\"w\",shape=[2*hidden, 20])\n",
    "        b = tf.get_variable(name=\"b\",shape=[20], initializer=tf.constant_initializer(0.))\n",
    "        logits_ = tf.nn.bias_add(tf.matmul(po_out, w), b)\n",
    "        logits = tf.nn.sigmoid(logits_)\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        # loss = tf.reduce_mean(tf.square(normalize_labels - logits))\n",
    "        loss = tf.losses.mean_squared_error(labels=labels, predictions=logits)\n",
    "    with tf.variable_scope(\"optimizer\"):\n",
    "        learning_rate = tf.get_variable(\"learning_rate\", shape=[], dtype=tf.float32, trainable=False)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate, epsilon=1e-6)\n",
    "        grads = optimizer.compute_gradients(loss)\n",
    "        gradients, variables = zip(*grads)\n",
    "        capped_grads, _ = tf.clip_by_global_norm(gradients, grad_clip)\n",
    "        train_op = optimizer.apply_gradients(zip(capped_grads, variables), global_step=global_step)\n",
    "    \n",
    "    # 执行sess\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "    sess_config.gpu_options.allow_growth = True\n",
    "    loss_save = 100.0\n",
    "    patience = 0\n",
    "    best_dev_f1 = 0.0\n",
    "    with tf.Session(config=sess_config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.assign(learning_rate,tf.constant(init_lr, dtype=tf.float32)))\n",
    "        # 准备batch\n",
    "        train_set = batch_generator(get_dataset(train_after_clean), batch_size, embedding_bin)\n",
    "        val_set = batch_generator(get_dataset(val_after_clean), batch_size, embedding_bin)\n",
    "        # train\n",
    "        print(\"Training...\")\n",
    "        for go in range(100000):\n",
    "            steps = sess.run(global_step) + 1\n",
    "            one_batch_ids, one_batch_inputs, one_batch_labels = next(train_set)\n",
    "            one_batch_labels_ = normalize(one_batch_labels)\n",
    "            feed = {inputs:one_batch_inputs, labels:one_batch_labels_, keep_prob:0.7}\n",
    "            train_loss, train_optimizer, train_logits = sess.run([loss,train_op,logits], feed_dict=feed)\n",
    "            if steps % 100 == 0:\n",
    "                # train_prediction = predict(train_logits)\n",
    "                # train_f1 = f1_score(train_prediction, one_batch_labels)\n",
    "                print(\"steps:{},train_loss:{:.6f}\".format(steps, float(train_loss)))\n",
    "            if steps % 500 == 0:\n",
    "                # 验证\n",
    "                val_prediction_dict = {}\n",
    "                val_truth_dict = {}\n",
    "                val_losses = []\n",
    "                for _ in range(15000 // batch_size + 1):\n",
    "                    one_val_ids, one_val_inputs, one_val_labels = next(val_set)\n",
    "                    one_val_labels_ = normalize(one_val_labels)\n",
    "                    feed_val = {inputs:one_val_inputs, labels:one_val_labels_, keep_prob:1.0}\n",
    "                    val_loss, val_logits = sess.run([loss,logits], feed_dict=feed)\n",
    "                    val_prediction = predict(val_logits)\n",
    "                    p_dict = {}\n",
    "                    t_dict = {}\n",
    "                    for ids, tr, pre in zip(one_val_ids, one_val_labels, val_prediction):\n",
    "                        p_dict[str(ids)] = pre\n",
    "                        t_dict[str(ids)] = tr\n",
    "                    val_prediction_dict.update(p_dict)\n",
    "                    val_truth_dict.update(t_dict)\n",
    "                    val_losses.append(val_loss)\n",
    "                val_loss = np.mean(val_losses)\n",
    "                val_f1 = f1_score_from_sklearn(val_prediction_dict, val_truth_dict)\n",
    "                print(\"steps:{}, val_loss:{:.6f}, f1_score:{:.5f}\".format(steps, val_loss, val_f1))\n",
    "                # 学习率\n",
    "                if val_loss < loss_save:\n",
    "                    loss_save = val_loss\n",
    "                    patience = 0\n",
    "                else:\n",
    "                    patience += 1\n",
    "                if patience >= 3:\n",
    "                    init_lr /= 2.0\n",
    "                    loss_save = val_loss\n",
    "                    patience = 0\n",
    "                sess.run(tf.assign(learning_rate,tf.constant(init_lr, dtype=tf.float32)))\n",
    "                # 保存模型\n",
    "                if val_f1 > best_dev_f1:\n",
    "                    best_dev_f1 = val_f1\n",
    "                    filename = os.path.join(save_dir, \"model_{}_f1_{:.5f}.ckpt\".format(steps, best_dev_f1))\n",
    "                    saver.save(sess, filename)\n",
    "    print(\"finished!\")\n",
    "\n",
    "#train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试模块，读取模型，测试，生成提交文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing ...\n",
      "INFO:tensorflow:Restoring parameters from ./model/model_1500_f1_0.35543.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/235 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuhaitao/software/Python3/lib/python3.5/site-packages/jieba/__init__.py\", line 152, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpizwtt8jr' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.973 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "100%|██████████| 235/235 [02:03<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "测试模块\n",
    "\n",
    "@author: yuhaitao\n",
    "\"\"\"\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "def test(batch_size=64):\n",
    "    # 读取test数据\n",
    "    word2vec_model = word2vec.load(embedding_bin)\n",
    "    df = load_data_from_csv(test_file)\n",
    "    test_df = load_data_from_csv(test_after_clean)\n",
    "    # test\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "    sess_config.gpu_options.allow_growth = True\n",
    "    print(\"testing ...\")\n",
    "    with tf.Session(config=sess_config) as sess:\n",
    "        saver = tf.train.import_meta_graph(tf.train.latest_checkpoint(save_dir) + \".meta\")\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "        inputs = tf.get_default_graph().get_tensor_by_name(\"inputs:0\")\n",
    "        keep_prob = tf.get_default_graph().get_tensor_by_name(\"keep_prob:0\")\n",
    "        logits = tf.get_default_graph().get_tensor_by_name(\"fully_connect/Sigmoid:0\")\n",
    "        prediction = []\n",
    "        for i in tqdm(range(15000 // batch_size + 1)):\n",
    "            one_batch_inputs = np.zeros(shape=[batch_size, content_limit, 100], dtype=np.float32)\n",
    "            for j in range(batch_size):\n",
    "                if i*batch_size + j < 15000:\n",
    "                    one = test_df.iloc[i*batch_size + j][\"content\"]\n",
    "                else:\n",
    "                    one = \"牛肉\"\n",
    "                one = list(jieba.cut(one))\n",
    "                if len(one) > content_limit:\n",
    "                    one = one[:content_limit]\n",
    "                for index in range(len(one)):\n",
    "                    if one[index] in word2vec_model:\n",
    "                        one_batch_inputs[j,index,:] = word2vec_model[one[index]]\n",
    "            feed = {inputs:one_batch_inputs, keep_prob:1.0}\n",
    "            logits_ = sess.run(logits, feed_dict=feed)\n",
    "            predict_logits = predict(logits_)\n",
    "            # 与原content合成输出格式\n",
    "            for j in range(batch_size):\n",
    "                pre = []\n",
    "                if i*batch_size + j < 15000:\n",
    "                    predict_id = df.iloc[i*batch_size+j][\"id\"]\n",
    "                    predict_content = df.iloc[i*batch_size+j][\"content\"]\n",
    "                    pre.append(predict_id)\n",
    "                    pre.append(predict_content)\n",
    "                    pre += list(predict_logits[j,:])\n",
    "                    prediction.append(pre)\n",
    "        prediction_df = pd.DataFrame(data=prediction, columns=df.columns)\n",
    "        prediction_df.to_csv(\"predictions.csv\",index=None)\n",
    "    print(\"finished!\")\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
